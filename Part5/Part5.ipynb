{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tengfone/.pyenv/versions/3.8.5/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filePath): \n",
    "    with open (filePath, 'r') as f: \n",
    "        lines = f.readlines()\n",
    "        text = []\n",
    "        label = []\n",
    "        label_split = []\n",
    "        temp_arr = []\n",
    "        label_split_temp = []\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                l_split = l.strip().split(\" \")\n",
    "                te = \" \".join(l_split[:-1])\n",
    "                la = l_split[-1]\n",
    "                temp_arr.append((te, la))\n",
    "                label.append(l_split[-1])\n",
    "                label_split_temp.append(l_split[-1])\n",
    "            else: \n",
    "                text.append(temp_arr)\n",
    "                temp_arr = []\n",
    "                label_split.append(label_split_temp)\n",
    "                label_split_temp = []\n",
    "    return text, label, label_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing of the loaded CSV File from the Part5_Preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filepath, lan):\n",
    "    \n",
    "    em_arr = pd.read_csv(filepath, sep=',',header=None)\n",
    "    em_arr = em_arr.T\n",
    "    em_arr = em_arr.drop([0, 2])\n",
    "    em_arr.columns = em_arr.iloc[0]\n",
    "    em_arr = em_arr[1:]\n",
    "    if lan == 'EN':\n",
    "        em_arr = em_arr.reindex([5,6,11,12,4,7,9,3,16,10,8,19,20,18,21,17,13,22,23,14,15])\n",
    "    elif lan == 'CN':\n",
    "        em_arr = em_arr.reindex([4,6,3,7,8,5,9])\n",
    "    else: \n",
    "         em_arr = em_arr.reindex([6,3,4,7,5,9,8])\n",
    "    header_list = list(em_arr.columns.values)\n",
    "    em = em_arr.drop(['Text'], axis=1).values\n",
    "    em = em.astype('float64')\n",
    "    header_list = header_list[1:]\n",
    "    return em, header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Each Label Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_index(filepath, header_list):\n",
    "    test = []\n",
    "    output_index = []\n",
    "    output_label = []\n",
    "    index = []\n",
    "    label = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                label.append(l.strip())\n",
    "                try: \n",
    "                    index.append(header_list.index(l.strip()))\n",
    "                except: \n",
    "                    index.append(header_list.index(\"#UNK#\"))\n",
    "            else: \n",
    "                output_index.append(index)\n",
    "                output_label.append(label)\n",
    "                index = []\n",
    "                label = []\n",
    "    return output_index, output_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Label to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_dict(text, label): \n",
    "    final_dict = {'START': len(text)}\n",
    "    label_counter = dict(Counter(label))\n",
    "    final_dict.update(label_counter)\n",
    "    final_dict['STOP'] = len(text)\n",
    "    return final_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Transmission Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tra_arr(d, a): \n",
    "    for seq in a:\n",
    "        seq.insert(0, \"START\")\n",
    "        seq.append(\"STOP\")\n",
    "    first_tra_arr = []\n",
    "    for ci, c in enumerate(list(d.keys())[1:-1]):\n",
    "        count_c = 0\n",
    "        for seq in a:\n",
    "            if seq[1] == c:\n",
    "                count_c += 1\n",
    "        first_tra_arr.append(count_c / len(a))\n",
    "    tra_arr = []\n",
    "    for wi, w in enumerate(list(d.keys())[:-1]):\n",
    "        arr2 = []\n",
    "        for ui, u in enumerate(list(d.keys())[1:-1]):\n",
    "            arr1 = []\n",
    "            count_wu = 0\n",
    "            for seq in a:\n",
    "                for c in range(len(seq)-1):\n",
    "                    if seq[c] == w and seq[c+1] == u:\n",
    "                        count_wu += 1\n",
    "            for vi, v in enumerate(list(d.keys())[1:]):\n",
    "                count_wuv = 0\n",
    "                for seq in a:\n",
    "                    for c in range(len(seq)-2):\n",
    "                        if seq[c] == w and seq[c+1] == u and seq[c+2] == v:\n",
    "                            count_wuv += 1\n",
    "                if count_wu == 0:\n",
    "                    arr1.append(-1)\n",
    "                else:\n",
    "                    arr1.append(count_wuv / count_wu)\n",
    "            arr2.append(arr1)\n",
    "        tra_arr.append(arr2)\n",
    "    return first_tra_arr, tra_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output File (Change output file name here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(first_tra_arr, tra_arr, em, final_dict, index, label):\n",
    "    seq = []\n",
    "    for ci, i in enumerate(index): \n",
    "        seq_temp = vertibi_algo(first_tra_arr, tra_arr, em, final_dict, i)\n",
    "        temp_arr = []\n",
    "        for cj, j in enumerate(seq_temp): \n",
    "            temp_arr.append(\"{} {}\".format(label[ci][cj],j))\n",
    "        seq.append(temp_arr)\n",
    "\n",
    "    # Change output file name\n",
    "    with open ('test.p5.out', 'w') as f: \n",
    "        for i in seq: \n",
    "            for j in i: \n",
    "                f.write(j+\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Order HMM Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertibi_algo(first_tra_arr, tra_arr, em_arr, d, n): \n",
    "    # forward \n",
    "    rst = []\n",
    "    pi_arr = []\n",
    "    dy_arr = []\n",
    "    maxidx_arr = []\n",
    "    \n",
    "    for ci, c in enumerate (n):\n",
    "        if ci == 0:\n",
    "            for i in range(len(em_arr)):\n",
    "                dy_arr.append(first_tra_arr[i] * em_arr[i][n[ci]])\n",
    "        else:\n",
    "            temp_arr = []\n",
    "            tempdy_arr = []\n",
    "            tempmaxidx_arr = []\n",
    "            for i in range(len(em_arr)): # for each node\n",
    "                node_arr = []\n",
    "                for j in range(len(em_arr)): # link to each previous node\n",
    "                    if ci == 1:\n",
    "                        node_arr.append(dy_arr[j] * tra_arr[ci-1][j][i] * em_arr[i][n[ci]])\n",
    "                    else:\n",
    "                        node_arr.append(dy_arr[j] * tra_arr[maxidx_arr[j]+1][j][i] * em_arr[i][n[ci]])\n",
    "                temp_arr.append(node_arr)\n",
    "                tempdy_arr.append(max(node_arr))\n",
    "                tempmaxidx_arr.append(node_arr.index(max(node_arr)))\n",
    "            pi_arr.append(temp_arr)\n",
    "            dy_arr = tempdy_arr\n",
    "            maxidx_arr = tempmaxidx_arr\n",
    "\n",
    "    # backward\n",
    "    maxidx = dy_arr.index(max(dy_arr))\n",
    "    rst.append(list(d.keys())[maxidx+1])\n",
    "\n",
    "    for i in range(len(n)+1):\n",
    "        if len(pi_arr) != 0:\n",
    "            pop_arr = pi_arr.pop(-1)\n",
    "            check_arr = pop_arr[maxidx]\n",
    "            maxidx = check_arr.index(max(check_arr))\n",
    "            rst.append(list(d.keys())[maxidx+1])\n",
    "    \n",
    "    return list(reversed(rst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Change File Path Here \n",
    "pre_filepath = '/Users/tengfone/Desktop/ML_Project/Part5/TEST.csv'\n",
    "devin_filepath = '/Users/tengfone/Desktop/ML_Project/data/test_data/test.in'\n",
    "train_filepath = '/Users/tengfone/Desktop/ML_Project/data/EN(2)/train'\n",
    "\n",
    "text, label, label_split = readFile(train_filepath)\n",
    "final_dict = get_label_dict(text, label)\n",
    "first_tra_arr, tra_arr = get_tra_arr(final_dict, label_split)\n",
    "em, header_list = preprocess(pre_filepath, 'EN') # Indicate if it is EN/CN/SG\n",
    "index, output_label = get_label_index(devin_filepath, header_list)\n",
    "\n",
    "# Change output file name in get_output function\n",
    "print(get_output(first_tra_arr, tra_arr, em, final_dict, index, output_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
